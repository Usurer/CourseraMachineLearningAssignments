{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 17:54:45.989681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# def sigmoid_derivative(z):\n",
    "#     return np.exp(-z) / ((1 + np.exp(-z)) ** 2)\n",
    "\n",
    "def sigmoid_derivative_2(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def cost_squared(y_expected, y_predicted):\n",
    "    return (y_expected - y_predicted) ** 2 / 2\n",
    "\n",
    "def cost_squared_derivative(y_expected, y_predicted):\n",
    "    return -(y_expected - y_predicted)\n",
    "\n",
    "def cost_inner(y_expected, y_predicted):\n",
    "    return y_expected * np.log(y_predicted) + (1 - y_expected) * np.log(1 - y_predicted)\n",
    "\n",
    "def cost_derivative(y_expected, y_predicted):\n",
    "    top = y_expected - y_predicted\n",
    "    bottom = y_predicted * (1 - y_predicted)\n",
    "    return top / bottom\n",
    "\n",
    "# I expect x_train to be an array of [x_1, x_2] vectors\n",
    "# y_train is an array of scalar values\n",
    "# W_A, W_B and W_C are [w1, w2] vectors\n",
    "def model(x_train, y_train, W_A, W_B, W_C, training_rate):    \n",
    "    B_A = B_B = B_C = 0\n",
    "    m = x_train.shape[0]\n",
    "    J = 0\n",
    "    dwa = np.zeros(2)\n",
    "    dwb = np.zeros(2)\n",
    "    dwc = np.zeros(2)\n",
    "\n",
    "    for i in range(m):\n",
    "        a0_1 = x_train[i][0]\n",
    "        a0_2 = x_train[i][1]\n",
    "\n",
    "        z1_1 = W_A[0] * a0_1 + W_A[1] * a0_2 + B_A\n",
    "        z1_2 = W_B[0] * a0_1 + W_B[1] * a0_2 + B_B\n",
    "\n",
    "        dz1_1_da0_1 = W_A[0]\n",
    "        dz1_1_da0_2 = W_A[1]\n",
    "        dz1_2_db0_1 = W_B[0]\n",
    "        dz1_2_db0_2 = W_B[1]\n",
    "\n",
    "        a1_1 = sigmoid(z1_1)\n",
    "        a1_2 = sigmoid(z1_2)\n",
    "\n",
    "        # da1_1_z1_1 = sigmoid_derivative(z1_1)\n",
    "        # da1_2_z1_2 = sigmoid_derivative(z1_2)\n",
    "        da1_1_z1_1 = sigmoid_derivative_2(a1_1)\n",
    "        da1_2_z1_2 = sigmoid_derivative_2(a1_2)\n",
    "\n",
    "        z2_1 = W_C[0] * a1_1 + W_C[1] * a1_2 + B_C\n",
    "\n",
    "        dz2_1_a1_1 = W_C[0]\n",
    "        dz2_1_a1_2 = W_C[1]\n",
    "\n",
    "        dz2_1_w_c0 = a1_1\n",
    "        dz2_1_w_c1 = a1_2\n",
    "\n",
    "        a2_1 = sigmoid(z2_1)\n",
    "        \n",
    "        # da2_1_z2_1 = sigmoid_derivative(z2_1)\n",
    "        da2_1_z2_1 = sigmoid_derivative_2(a2_1)\n",
    "\n",
    "        # diff = -1 * cost_inner(y_train[i], a2_1)\n",
    "        # diff = a2_1 - y_train[i]\n",
    "        # J = J + diff / m\n",
    "        # dDiff_da2_1 = cost_derivative(y_train[i], a2_1)\n",
    "        # dDiff_da2_1 = diff\n",
    "\n",
    "        # lets try squated error\n",
    "        # diff_sq = cost_squared(y_train[i], a2_1)\n",
    "        # J = J + diff_sq / m\n",
    "\n",
    "        diff_plain = a2_1 - y_train[i]\n",
    "        J = J + diff_plain / m\n",
    "\n",
    "        dj_squared = cost_squared_derivative(y_train[i], a2_1)\n",
    "        dDiff_da2_1 = dj_squared\n",
    "\n",
    "        dj_dwc_0 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_w_c0\n",
    "        dj_dwc_1 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_w_c1\n",
    "        dwc[0] = dwc[0] + dj_dwc_0 / m\n",
    "        dwc[1] = dwc[1] + dj_dwc_1 / m\n",
    "\n",
    "        dj_dwb_0 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_a1_1 * da1_1_z1_1 * dz1_2_db0_1\n",
    "        dj_dwb_1 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_a1_2 * da1_2_z1_2 * dz1_2_db0_2\n",
    "        dwb[0] = dwb[0]  + dj_dwb_0 / m\n",
    "        dwb[1] = dwb[1]  + dj_dwb_1 / m\n",
    "\n",
    "        dj_dwa_0 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_a1_1 * da1_1_z1_1 * dz1_1_da0_1\n",
    "        dj_dwa_1 = dDiff_da2_1 * da2_1_z2_1 * dz2_1_a1_2 * da1_2_z1_2 * dz1_1_da0_2\n",
    "        dwa[0] = dwa[0]  + dj_dwa_0 / m\n",
    "        dwa[1] = dwa[1]  + dj_dwa_1 / m\n",
    "\n",
    "    return (W_A - training_rate * dwa, W_B - training_rate * dwb, W_C - training_rate * dwc, J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x1, x2, W_A, W_B, W_C):    \n",
    "    B_A = B_B = B_C = 0\n",
    "\n",
    "    a0_1 = x1\n",
    "    a0_2 = x2\n",
    "\n",
    "    z1_1 = W_A[0] * a0_1 + W_A[1] * a0_2 + B_A\n",
    "    z1_2 = W_B[0] * a0_1 + W_B[1] * a0_2 + B_B\n",
    "\n",
    "    a1_1 = sigmoid(z1_1)\n",
    "    a1_2 = sigmoid(z1_2)\n",
    "\n",
    "    z2_1 = W_C[0] * a1_1 + W_C[1] * a1_2 + B_C\n",
    "\n",
    "    a2_1 = sigmoid(z2_1)\n",
    "\n",
    "    return a2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coffee_dataset():\n",
    "    \"\"\"\n",
    "    Copied from lab_coffee_utils\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2)\n",
    "    X = rng.random(400).reshape(-1,2)\n",
    "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "    Y = np.zeros(len(X))\n",
    "    \n",
    "    i=0\n",
    "    for t,d in X:\n",
    "        y = -3/(260-175)*t + 21\n",
    "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        i += 1\n",
    "\n",
    "    return (X, Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_coffee_dataset()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.64939025  4.72355177] [1.71382025 0.50859957] [-0.68456199 -1.05665233] [0.09039993]\n"
     ]
    }
   ],
   "source": [
    "W_A = np.random.randn(2) #np.array([0.3092, 0.134])\n",
    "W_B = np.random.randn(2) #np.array([0.233, 0.7862])\n",
    "W_C = np.random.randn(2) #np.array([0.3674, 0.63])\n",
    "J = 0\n",
    "\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(x_train)\n",
    "\n",
    "sample_data = norm_l(x_train).numpy()\n",
    "\n",
    "epochs = 5000\n",
    "training_rate = 0.01\n",
    "\n",
    "for e in range(epochs):\n",
    "    W_A, W_B, W_C, J = model(sample_data, y_train, W_A, W_B, W_C, training_rate)\n",
    "    # if e % 100 == 0:\n",
    "    #     print(f'J[{e}] = {J}')\n",
    "\n",
    "print(W_A, W_B, W_C, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49970873924664855 0.1491587599901085\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17] # negative example\n",
    "])   \n",
    "\n",
    "sample1 = norm_l(np.array([[200,0.1]])).numpy()[0]\n",
    "result1 = predict(sample1[0], sample1[1], W_A, W_B, W_C)\n",
    "\n",
    "sample2 = norm_l(np.array([[200,140]])).numpy()[0]\n",
    "result2 = predict(sample2[0], sample2[1], W_A, W_B, W_C)\n",
    "\n",
    "print(result1, result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
