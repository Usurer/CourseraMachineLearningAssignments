{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 09:07:10.827876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip( z, -500, 500 ) #protect against overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def init_weights(weights_amount, neurons_amount, randomize = False):\n",
    "    '''\n",
    "    Params:\n",
    "        weights_amount  : how many weights do we have for one neuron, equals to amount of inputs\n",
    "        neurons_amount  : how many neurons do we have in a layer\n",
    "    Returns:\n",
    "        [M x N] matrix of ones or randoms, where\n",
    "            M (row count): the amount of neurons\n",
    "            N (col count): the amount of weighs\n",
    "    '''\n",
    "    size = (neurons_amount, weights_amount)\n",
    "    return np.random.default_rng().standard_normal(size) if randomize else np.ones(size)\n",
    "\n",
    "def init_biases(neurons_amount, randomize = False):\n",
    "    size = (neurons_amount, 1)\n",
    "    return np.random.default_rng().standard_normal(size) if randomize else np.ones(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coffee_dataset():\n",
    "    \"\"\"\n",
    "    Copied from lab_coffee_utils\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2)\n",
    "    X = rng.random(400).reshape(-1,2)\n",
    "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "    Y = np.zeros(len(X))\n",
    "    \n",
    "    i=0\n",
    "    for t,d in X:\n",
    "        y = -3/(260-175)*t + 21\n",
    "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        i += 1\n",
    "\n",
    "    return (X, Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 09:07:24.112121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X,Y = create_coffee_dataset()\n",
    "\n",
    "# Create normalization layer\n",
    "# Can be used to normalized other values, not from existing dataset\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer = namedtuple(\"Layer\", [\"W\", \"b\", \"a_in\", \"a_out\", \"z\"])\n",
    "LayerBack = namedtuple(\"LayerBack\", [\"dw\", \"da\", \"db\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_layer(W, b, a_in) -> Layer:\n",
    "    function_input = np.matmul(W, a_in) + b\n",
    "    function_out = sigmoid(function_input)\n",
    "    return Layer(W, b, a_in, function_out, function_input)\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return sigmoid(a) * (1 - sigmoid(a))\n",
    "\n",
    "def layer_derivatives(l: Layer) -> LayerBack:    \n",
    "    dz = sigmoid_derivative(l.z)    \n",
    "    print(f'A_In = {l.a_in.shape}, W = {l.W.shape}, dz = {dz.shape}')\n",
    "\n",
    "    dw = l.a_in * dz\n",
    "    da = (l.W.T) * dz\n",
    "    db = dz\n",
    "    return LayerBack(dw, da, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200)\n",
      "A_In = (3, 200), W = (1, 3), dz = (1, 200)\n",
      "A_In = (2, 200), W = (3, 2), dz = (3, 200)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,200) (3,200) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(diff\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m layer2_back \u001b[39m=\u001b[39m layer_derivatives(layer2)\n\u001b[0;32m---> 24\u001b[0m layer1_back \u001b[39m=\u001b[39m layer_derivatives(layer1)\n\u001b[1;32m     25\u001b[0m W1 \u001b[39m=\u001b[39m W1 \u001b[39m-\u001b[39m diff \u001b[39m*\u001b[39m layer2_back\u001b[39m.\u001b[39mdw\n\u001b[1;32m     26\u001b[0m B1 \u001b[39m=\u001b[39m B1 \u001b[39m-\u001b[39m diff \u001b[39m*\u001b[39m layer2_back\u001b[39m.\u001b[39mdb\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mlayer_derivatives\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     10\u001b[0m dz \u001b[39m=\u001b[39m sigmoid_derivative(l\u001b[39m.\u001b[39mz)    \n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mA_In = \u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m.\u001b[39ma_in\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, W = \u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, dz = \u001b[39m\u001b[39m{\u001b[39;00mdz\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m dw \u001b[39m=\u001b[39m l\u001b[39m.\u001b[39;49ma_in \u001b[39m*\u001b[39;49m dz\n\u001b[1;32m     14\u001b[0m da \u001b[39m=\u001b[39m (l\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mT) \u001b[39m*\u001b[39m dz\n\u001b[1;32m     15\u001b[0m db \u001b[39m=\u001b[39m dz\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,200) (3,200) "
     ]
    }
   ],
   "source": [
    "# First layer, 3 neurons, 2 inputs X1 and X2\n",
    "W0 = init_weights(2, 3, True)\n",
    "B0 = init_biases(3, True)\n",
    "\n",
    "# Second layer, 1 neuron, 3 inputs from previous layer\n",
    "W1 = init_weights(3, 1, True)\n",
    "B1 = init_biases(1, True)\n",
    "\n",
    "# normalize inputs and convert to numpy\n",
    "sample_data = norm_l(X).numpy().T\n",
    "\n",
    "# print(W0)\n",
    "# print(B0)\n",
    "# print(sample_data)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "    layer1 = execute_layer(W0, B0, sample_data)\n",
    "    layer2 = execute_layer(W1, B1, layer1.a_out)\n",
    "    diff = Y.T - layer2.a_out\n",
    "    print(diff.shape)\n",
    "    layer2_back = layer_derivatives(layer2)\n",
    "    layer1_back = layer_derivatives(layer1)\n",
    "    W1 = W1 - diff * layer2_back.dw\n",
    "    B1 = B1 - diff * layer2_back.db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [ 5, 6]\n",
    "])\n",
    "b = np.array([[1], [2], [3]])\n",
    "c = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "print(b.shape, c.shape)\n",
    "c * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10, 100],\n",
       "       [ 20, 200],\n",
       "       [ 30, 300]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1], [2], [3]])\n",
    "b = np.array([10, 100])\n",
    "b * a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
