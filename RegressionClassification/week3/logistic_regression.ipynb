{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(W: list, X: list, b: float):\n",
    "    '''\n",
    "    Sigmoid function for a single set of data\n",
    "    \n",
    "    Parameters:\n",
    "        W: a list of weights\n",
    "        X: a list of input variables\n",
    "        b: a single number\n",
    "    \n",
    "    Returns:\n",
    "        Sigmoid func f(z) where z = W * X + b\n",
    "    '''\n",
    "    z = np.dot(W, X) + b\n",
    "    return 1 / (1  + np.exp(-z))\n",
    "\n",
    "def derivatives(X: np.ndarray, Y: list, W: list, b: float) -> tuple[list, float]:\n",
    "    '''\n",
    "    Calculates the Cost Function (J) derivatives by W and B variables\n",
    "    dJ/dW = 1/m * SUM((f(x_i) - y_i) * x_i), where i = [1, m] and iterates over the learning dataset\n",
    "    dJ/db = 1/m * SUM(f(x_i) - y_i)\n",
    "    \n",
    "    Parameters:\n",
    "        X: 2d array that contains a list of input variables X for every data entry\n",
    "        Y: a list of target variables, one for every data entry\n",
    "        W: a list of weights W, these values will apply to all data entries\n",
    "        b: a value of weight b, this value will apply to all data entries\n",
    "        \n",
    "    Returns:\n",
    "        dJ/dW, dJ/db: where first is a list of values, since W is a list; second is a single value\n",
    "    '''\n",
    "\n",
    "    # TODO: I'm using an NDArray size method, but I'd like Y to be a more generic list, can we use list len() method here?\n",
    "    m = Y.size #number of training set examples\n",
    "    sum_db = 0 # dJ/db is a single value\n",
    "    sum_dw = np.zeros(X.shape[1]) # dJ/dW is a list of values, depends on the number of variables x\n",
    "    \n",
    "    for i in range(m):\n",
    "        x: list = X[i] # Take the set of variables for the i-th training example\n",
    "        y: float = Y[i] # Take the i-th output\n",
    "        f = sigmoid(W, x, b) # Calculate prediction for given W, x and b\n",
    "        prediction_error = (f - y)\n",
    "        sum_db = sum_db + prediction_error\n",
    "        sum_dw = sum_dw + prediction_error * x\n",
    "    \n",
    "    return sum_dw / m, sum_db / m\n",
    "\n",
    "def run_descent(X: np.ndarray, Y: list, W0: list, b0: float) -> tuple[list, float]:\n",
    "    '''\n",
    "    Run gradient descent for a given training dataset of X, Y and staring model weights W0 and b0\n",
    "    \n",
    "    Parameters:\n",
    "        X: training dataset variables, a list of lists\n",
    "        Y: training dataset targets, a list\n",
    "        W0: a list of initial weights W, the length corresponds to a number of variables x\n",
    "        b0: initial value for a weight b\n",
    "        \n",
    "    Returns:\n",
    "        W, b: weights for a prediction model function\n",
    "    '''\n",
    "    W = W0\n",
    "    b = b0\n",
    "    alpha = 0.001\n",
    "    steps = 1 * 10 ** 5\n",
    "\n",
    "    for _ in range(steps):\n",
    "        dJ_dW, dJ_db = derivatives(X, Y, W, b)\n",
    "        W = W - alpha * dJ_dW\n",
    "        b = b - alpha * dJ_db\n",
    "\n",
    "    return W, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = load_data(\"data/ex2data1.txt\")\n",
    "\n",
    "# Let's take a look at the data\n",
    "print(f'The dataset has {X.shape[1]} x variables')\n",
    "print(f'The dataset has total amount of {X.shape[0]} samples')\n",
    "\n",
    "plot_data(X, y[:], pos_label=\"Admitted\", neg_label=\"Not admitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = [1, 1] # Initial values for weights w, there are 2 values because we have 2 variables X in the dataset\n",
    "b0 = 1\n",
    "\n",
    "# Getting weights for our prediction model\n",
    "# To get the model prediction we can run sigmoid() function \n",
    "# Plus we need to define the threshold - whether the model result considered to be 0 or 1\n",
    "W, b = run_descent(X, y, W0, b0)\n",
    "\n",
    "print(f'Model weights are [{W}] for W and {b} for b')\n",
    "\n",
    "\n",
    "# Let's feed the training data to the model and see results\n",
    "training_set_size = X.shape[0]\n",
    "predictions = np.zeros(training_set_size)\n",
    "threshold = 0.5\n",
    "\n",
    "for i in range(training_set_size):\n",
    "    sig = sigmoid(W, X[i], b)\n",
    "    prediction = 1 if sig > threshold else 0\n",
    "    predictions[i] = prediction\n",
    "    print(f'known vs predicted: {y[i]} | {prediction}')\n",
    "\n",
    "\n",
    "plot_data(X, predictions[:], pos_label=\"Admitted\", neg_label=\"Not admitted\")\n",
    "\n",
    "print('Train Accuracy: %f'%(np.mean(predictions == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're supposed to deal with Feature Engineering and Regularization\n",
    "# Let's take a look at the dataset\n",
    "\n",
    "X, y = load_data(\"data/ex2data2.txt\")\n",
    "plot_data(X, y[:], pos_label=\"Good\", neg_label=\"Bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does our model handle it? Let's try!\n",
    "\n",
    "W, b = run_descent(X, y, W0, b0)\n",
    "\n",
    "training_set_size = X.shape[0]\n",
    "predictions = np.zeros(training_set_size)\n",
    "threshold = 0.5\n",
    "\n",
    "for i in range(training_set_size):\n",
    "    sig = sigmoid(W, X[i], b)\n",
    "    prediction = 1 if sig > threshold else 0\n",
    "    predictions[i] = prediction\n",
    "\n",
    "\n",
    "plot_data(X, predictions[:], pos_label=\"Good\", neg_label=\"Bad\")\n",
    "\n",
    "# As expected the prediction results are unsatisfactory.\n",
    "# As far as I understand, that's because for our sigmoid function we use a linear function\n",
    "# y = w * x + b\n",
    "# Or, since there are two variables in the dataset:\n",
    "# y = w1 * x1 + w2 * x2 + b\n",
    "# However, the decision boundary of the training dataset resembles a circle, so maybe we'll do better \n",
    "# by creating 'artificial' variables x1^2 and x2^2\n",
    "# Please mind that by now it's just my speculation and it seems like we're expected to do a much heavier feature engineering for this task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_variables(X):\n",
    "    '''\n",
    "    An extremely clumsy way to add some normalization and feature engineering.\n",
    "    I'm adding 3 more variables to existing x_1 and x_2: x_1^2, x_2^2 and x_1 * x_2\n",
    "    Then I calculate averages for these variables \n",
    "    Then for every sample from the training data set I transform variables as following:\n",
    "        x_1     -> x_1/avg(x_1)\n",
    "        x_1^2   -> x_1^2/avg(x_1^2)\n",
    "        and so on\n",
    "    \n",
    "    I'm not familiar with array manipulations in Python and NumPy,\n",
    "    so I just do these operations in a few runs\n",
    "        \n",
    "    Returns:\n",
    "        NDArray of m arrays, with 5 values in every array,\n",
    "        where m is the amount of samples in the training dataset (118 in our case)\n",
    "    '''\n",
    "    n = 2 # amount of existing variables   \n",
    "    m = X.shape[0] # amount of samples\n",
    "    \n",
    "    engineered = [\n",
    "        X[:, 0]**2, \n",
    "        X[:, 1]**2, \n",
    "        X[:, 0] * X[:, 1]\n",
    "    ]\n",
    "    \n",
    "    averages = []\n",
    "    result = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_i = X[:, i]\n",
    "        x_avg = np.average(x_i)\n",
    "        averages.append(x_avg)\n",
    "\n",
    "    \n",
    "    for item in engineered:\n",
    "        e_i = item\n",
    "        e_avg = np.average(e_i)\n",
    "        averages.append(e_avg)\n",
    "\n",
    "    averages = np.array(averages)\n",
    "    \n",
    "    for i in range(m):\n",
    "        current_sample = np.array([*X[i], engineered[0][i], engineered[1][i], engineered[2][i]])\n",
    "        normalized = current_sample / averages\n",
    "        result.append(normalized)\n",
    "        \n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "X, y = load_data(\"data/ex2data2.txt\")\n",
    "normalization_data = normalize_variables(X)\n",
    "\n",
    "X_engineered = np.array(normalization_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = [1, 1, 1, 1, 1] # Initial values for weights w, there are 2 values because we have 2 variables X in the dataset\n",
    "b0 = 1\n",
    "\n",
    "W, b = run_descent(X_engineered, y, W0, b0)\n",
    "\n",
    "training_set_size = X.shape[0]\n",
    "predictions = np.zeros(training_set_size)\n",
    "threshold = 0.5\n",
    "\n",
    "for i in range(training_set_size):\n",
    "    sig = sigmoid(W, X_engineered[i], b)\n",
    "    prediction = 1 if sig > threshold else 0\n",
    "    predictions[i] = prediction\n",
    "    \n",
    "print('Train Accuracy: %f'%(np.mean(predictions == y) * 100))\n",
    "\n",
    "plot_data(X, predictions[:], pos_label=\"Good\", neg_label=\"Bad\")\n",
    "\n",
    "# I did not expect this, considering how bad the code is\n",
    "# But the plot of the training data feed to  my model looks similar to the \n",
    "# training data set. That is amazing, really. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
